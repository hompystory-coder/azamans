#!/usr/bin/env python3
"""
ğŸ“¦ AI ëª¨ë¸ ìë™ ë‹¤ìš´ë¡œë“œ ë° ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸
ë¡œì»¬ PCì—ì„œ ì‡¼ì¸  ìƒì„±ì— í•„ìš”í•œ ëª¨ë“  ì˜¤í”ˆì†ŒìŠ¤ AI ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
"""

import os
import sys
from pathlib import Path
import subprocess
from loguru import logger
import torch

# í”„ë¡œì íŠ¸ ë£¨íŠ¸
BASE_DIR = Path(__file__).parent.parent
MODELS_DIR = BASE_DIR / "models"
MODELS_DIR.mkdir(parents=True, exist_ok=True)

def check_system():
    """ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ í™•ì¸"""
    logger.info("ğŸ” Checking system requirements...")
    
    # Python ë²„ì „
    python_version = sys.version_info
    logger.info(f"   Python: {python_version.major}.{python_version.minor}.{python_version.micro}")
    
    if python_version < (3, 8):
        logger.error("âŒ Python 3.8 ì´ìƒì´ í•„ìš”í•©ë‹ˆë‹¤!")
        return False
    
    # PyTorch
    try:
        import torch
        logger.info(f"   PyTorch: {torch.__version__}")
        
        # CUDA í™•ì¸
        if torch.cuda.is_available():
            device_name = torch.cuda.get_device_name(0)
            vram = torch.cuda.get_device_properties(0).total_memory / 1024**3
            logger.info(f"   GPU: {device_name}")
            logger.info(f"   VRAM: {vram:.1f} GB")
            
            if vram < 6:
                logger.warning(f"âš ï¸ VRAMì´ {vram:.1f}GBë¡œ ê¶Œì¥ ì‚¬ì–‘(6GB ì´ìƒ)ë³´ë‹¤ ì ìŠµë‹ˆë‹¤.")
                logger.warning("   ì¼ë¶€ ê¸°ëŠ¥ì´ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        else:
            logger.warning("âš ï¸ CUDA ì‚¬ìš© ë¶ˆê°€ - CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤ (ëŠë¦´ ìˆ˜ ìˆìŒ)")
    except ImportError:
        logger.error("âŒ PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
        logger.info("   ì„¤ì¹˜: pip install torch torchvision torchaudio")
        return False
    
    # ë””ìŠ¤í¬ ê³µê°„ í™•ì¸
    import shutil
    total, used, free = shutil.disk_usage(MODELS_DIR)
    free_gb = free / 1024**3
    logger.info(f"   Free disk space: {free_gb:.1f} GB")
    
    if free_gb < 30:
        logger.error(f"âŒ ë””ìŠ¤í¬ ì—¬ìœ  ê³µê°„ì´ {free_gb:.1f}GBë¡œ ë¶€ì¡±í•©ë‹ˆë‹¤! (ìµœì†Œ 30GB í•„ìš”)")
        return False
    
    logger.info("âœ… System requirements check passed!")
    return True

def install_package(package: str):
    """Python íŒ¨í‚¤ì§€ ì„¤ì¹˜"""
    try:
        logger.info(f"ğŸ“¦ Installing {package}...")
        subprocess.check_call([sys.executable, "-m", "pip", "install", package, "-q"])
        logger.info(f"   âœ… {package} installed")
        return True
    except subprocess.CalledProcessError:
        logger.error(f"   âŒ Failed to install {package}")
        return False

def download_stable_diffusion():
    """Stable Diffusion XL ë‹¤ìš´ë¡œë“œ"""
    logger.info("ğŸ¨ Downloading Stable Diffusion XL...")
    logger.info("   Model: stabilityai/stable-diffusion-xl-base-1.0")
    logger.info("   Size: ~6.9 GB")
    logger.info("   This may take 10-30 minutes depending on your internet speed...")
    
    try:
        from diffusers import StableDiffusionXLPipeline
        
        # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ìºì‹œ ë””ë ‰í† ë¦¬ ì§€ì •)
        model_id = "stabilityai/stable-diffusion-xl-base-1.0"
        
        logger.info("   Downloading... (ì§„í–‰ ìƒí™©ì´ í‘œì‹œë©ë‹ˆë‹¤)")
        pipe = StableDiffusionXLPipeline.from_pretrained(
            model_id,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            variant="fp16" if torch.cuda.is_available() else None,
            cache_dir=MODELS_DIR,
            use_safetensors=True
        )
        
        logger.info("âœ… Stable Diffusion XL downloaded successfully!")
        
        # ë©”ëª¨ë¦¬ í•´ì œ
        del pipe
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ Failed to download Stable Diffusion XL: {str(e)}")
        return False

def download_coqui_tts():
    """Coqui TTS ë‹¤ìš´ë¡œë“œ"""
    logger.info("ğŸ™ï¸ Downloading Coqui TTS (XTTS-v2)...")
    logger.info("   Model: xtts-v2 (multilingual)")
    logger.info("   Size: ~2 GB")
    
    try:
        from TTS.api import TTS
        
        # TTS ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
        logger.info("   Downloading... (ìë™ìœ¼ë¡œ ìºì‹œë©ë‹ˆë‹¤)")
        tts = TTS(
            model_name="tts_models/multilingual/multi-dataset/xtts_v2",
            progress_bar=True
        )
        
        logger.info("âœ… Coqui TTS downloaded successfully!")
        logger.info(f"   Supported languages: {tts.languages if hasattr(tts, 'languages') else 'multilingual'}")
        
        del tts
        return True
        
    except Exception as e:
        logger.error(f"âŒ Failed to download Coqui TTS: {str(e)}")
        return False

def install_ollama():
    """Ollama ì„¤ì¹˜ ì•ˆë‚´ (LLMìš©)"""
    logger.info("ğŸ¤– Ollama (LLM) ì„¤ì¹˜ ì•ˆë‚´")
    logger.info("   OllamaëŠ” ë³„ë„ ì„¤ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    logger.info("")
    logger.info("   1. https://ollama.com/ ì—ì„œ ë‹¤ìš´ë¡œë“œ")
    logger.info("   2. ì„¤ì¹˜ í›„ ë‹¤ìŒ ëª…ë ¹ì–´ ì‹¤í–‰:")
    logger.info("      ollama pull llama3.1:8b")
    logger.info("")
    logger.info("   ë˜ëŠ” CLIë¡œ ì„¤ì¹˜:")
    
    import platform
    system = platform.system()
    
    if system == "Linux":
        logger.info("      curl -fsSL https://ollama.com/install.sh | sh")
    elif system == "Darwin":  # macOS
        logger.info("      brew install ollama")
    elif system == "Windows":
        logger.info("      PowerShellì—ì„œ: winget install Ollama.Ollama")
    
    logger.info("")
    logger.info("âš ï¸ OllamaëŠ” ì„ íƒì‚¬í•­ì…ë‹ˆë‹¤. AI ìŠ¤í¬ë¦½íŠ¸ ìƒì„± ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ë ¤ë©´ ì„¤ì¹˜í•˜ì„¸ìš”.")
    
    return True

def verify_models():
    """ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸ í™•ì¸"""
    logger.info("ğŸ” Verifying downloaded models...")
    
    models_found = []
    
    # Stable Diffusion í™•ì¸
    sd_path = MODELS_DIR / "models--stabilityai--stable-diffusion-xl-base-1.0"
    if sd_path.exists():
        models_found.append("âœ… Stable Diffusion XL")
    else:
        models_found.append("âŒ Stable Diffusion XL (not found)")
    
    # TTS í™•ì¸ (Hugging Face ìºì‹œ)
    import os
    hf_home = os.environ.get("HF_HOME", Path.home() / ".cache" / "huggingface")
    tts_path = Path(hf_home) / "hub"
    if tts_path.exists():
        tts_models = list(tts_path.glob("models--coqui--*"))
        if tts_models:
            models_found.append("âœ… Coqui TTS")
        else:
            models_found.append("âŒ Coqui TTS (not found)")
    else:
        models_found.append("âŒ Coqui TTS (not found)")
    
    # Ollama í™•ì¸
    try:
        result = subprocess.run(["ollama", "list"], capture_output=True, text=True)
        if "llama3.1" in result.stdout:
            models_found.append("âœ… Ollama LLaMA 3.1")
        else:
            models_found.append("âš ï¸ Ollama installed but LLaMA 3.1 not pulled")
    except FileNotFoundError:
        models_found.append("âŒ Ollama (not installed)")
    
    logger.info("")
    logger.info("ğŸ“Š Model Status:")
    for status in models_found:
        logger.info(f"   {status}")
    
    return True

def main():
    """ë©”ì¸ ì„¤ì¹˜ í”„ë¡œì„¸ìŠ¤"""
    logger.info("=" * 60)
    logger.info("ğŸš€ AI Shorts Generator - Model Installation")
    logger.info("=" * 60)
    logger.info("")
    
    # 1. ì‹œìŠ¤í…œ ì²´í¬
    if not check_system():
        logger.error("âŒ System requirements not met. Aborting.")
        return False
    
    logger.info("")
    logger.info("=" * 60)
    logger.info("ğŸ“¥ Starting model downloads...")
    logger.info("=" * 60)
    logger.info("")
    
    # 2. í•„ìˆ˜ íŒ¨í‚¤ì§€ í™•ì¸
    required_packages = {
        "diffusers": "diffusers>=0.25.0",
        "transformers": "transformers>=4.36.0",
        "TTS": "TTS>=0.22.0",
        "accelerate": "accelerate>=0.25.0",
    }
    
    for module, package in required_packages.items():
        try:
            __import__(module)
            logger.info(f"âœ… {module} already installed")
        except ImportError:
            logger.info(f"ğŸ“¦ {module} not found, installing...")
            if not install_package(package):
                logger.error(f"âŒ Failed to install {package}")
                return False
    
    logger.info("")
    
    # 3. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
    logger.info("â³ This will download ~10GB of AI models.")
    logger.info("   Please ensure you have:")
    logger.info("   - Good internet connection")
    logger.info("   - At least 30GB free disk space")
    logger.info("   - 30-60 minutes of time")
    logger.info("")
    
    input("Press Enter to continue or Ctrl+C to cancel...")
    logger.info("")
    
    # Stable Diffusion
    if not download_stable_diffusion():
        logger.error("âŒ Stable Diffusion download failed")
        return False
    
    logger.info("")
    
    # Coqui TTS
    if not download_coqui_tts():
        logger.error("âŒ Coqui TTS download failed")
        return False
    
    logger.info("")
    
    # Ollama ì•ˆë‚´
    install_ollama()
    
    logger.info("")
    logger.info("=" * 60)
    logger.info("ğŸ‰ Model installation process complete!")
    logger.info("=" * 60)
    logger.info("")
    
    # 4. ê²€ì¦
    verify_models()
    
    logger.info("")
    logger.info("âœ… You can now run the backend server:")
    logger.info("   cd backend")
    logger.info("   python app.py")
    logger.info("")
    
    return True

if __name__ == "__main__":
    try:
        success = main()
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        logger.warning("\nâš ï¸ Installation cancelled by user")
        sys.exit(1)
    except Exception as e:
        logger.error(f"\nâŒ Unexpected error: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
