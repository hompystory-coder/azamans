#!/usr/bin/env python3
"""
ğŸ¬ ë¹„ë””ì˜¤ ìƒì„± ëª¨ë“ˆ
AnimateDiff + Stable Diffusionì„ í™œìš©í•œ Image-to-Video ë³€í™˜
"""

import torch
from diffusers import (
    AnimateDiffPipeline,
    MotionAdapter,
    DDIMScheduler,
    StableDiffusionPipeline
)
from diffusers.utils import export_to_video
from pathlib import Path
from typing import Optional, List, Union
from loguru import logger
import time
from PIL import Image
import numpy as np

class VideoGenerator:
    """AnimateDiff ê¸°ë°˜ ë¹„ë””ì˜¤ ìƒì„±ê¸°"""
    
    def __init__(self, models_dir: Path, device: str = "cuda"):
        self.models_dir = models_dir
        self.device = device
        self.pipe = None
        self.motion_adapter = None
        self.loaded = False
        
    def load_model(
        self,
        base_model: str = "runwayml/stable-diffusion-v1-5",
        motion_adapter_id: str = "guoyww/animatediff-motion-adapter-v1-5-2"
    ):
        """AnimateDiff ëª¨ë¸ ë¡œë“œ
        
        Args:
            base_model: Stable Diffusion ë² ì´ìŠ¤ ëª¨ë¸
            motion_adapter_id: AnimateDiff ëª¨ì…˜ ì–´ëŒ‘í„°
        """
        try:
            logger.info(f"ğŸ¬ Loading AnimateDiff...")
            logger.info(f"   Base model: {base_model}")
            logger.info(f"   Motion adapter: {motion_adapter_id}")
            start_time = time.time()
            
            # Motion Adapter ë¡œë“œ
            logger.info("   Loading motion adapter...")
            self.motion_adapter = MotionAdapter.from_pretrained(
                motion_adapter_id,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                cache_dir=self.models_dir
            )
            
            # AnimateDiff íŒŒì´í”„ë¼ì¸ ë¡œë“œ
            logger.info("   Loading AnimateDiff pipeline...")
            self.pipe = AnimateDiffPipeline.from_pretrained(
                base_model,
                motion_adapter=self.motion_adapter,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                cache_dir=self.models_dir
            )
            
            # ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
            self.pipe.scheduler = DDIMScheduler.from_config(
                self.pipe.scheduler.config,
                beta_schedule="linear",
                clip_sample=False
            )
            
            # GPUë¡œ ì´ë™
            if self.device == "cuda":
                self.pipe = self.pipe.to(self.device)
                
                # ë©”ëª¨ë¦¬ ìµœì í™”
                self.pipe.enable_vae_slicing()
                self.pipe.enable_model_cpu_offload()  # CPU ì˜¤í”„ë¡œë“œ (VRAM ì ˆì•½)
                
                try:
                    self.pipe.enable_xformers_memory_efficient_attention()
                    logger.info("   âœ… xformers enabled")
                except:
                    logger.warning("   âš ï¸ xformers not available")
            
            load_time = time.time() - start_time
            logger.info(f"âœ… AnimateDiff loaded in {load_time:.1f}s")
            self.loaded = True
            
        except Exception as e:
            logger.error(f"âŒ Failed to load AnimateDiff: {str(e)}")
            raise
    
    def text_to_video(
        self,
        prompt: str,
        negative_prompt: Optional[str] = None,
        num_frames: int = 16,
        fps: int = 8,
        width: int = 512,
        height: int = 512,
        num_inference_steps: int = 25,
        guidance_scale: float = 7.5,
        seed: Optional[int] = None,
        output_path: Optional[Path] = None
    ) -> str:
        """í…ìŠ¤íŠ¸ë¡œë¶€í„° ë¹„ë””ì˜¤ ìƒì„±
        
        Args:
            prompt: ìƒì„± í”„ë¡¬í”„íŠ¸
            negative_prompt: ë„¤ê±°í‹°ë¸Œ í”„ë¡¬í”„íŠ¸
            num_frames: í”„ë ˆì„ ìˆ˜ (16-24 ê¶Œì¥)
            fps: ì´ˆë‹¹ í”„ë ˆì„ (8-12 ê¶Œì¥)
            width: ë¹„ë””ì˜¤ ë„ˆë¹„
            height: ë¹„ë””ì˜¤ ë†’ì´
            num_inference_steps: ì¶”ë¡  ìŠ¤í…
            guidance_scale: ê°€ì´ë˜ìŠ¤ ìŠ¤ì¼€ì¼
            seed: ëœë¤ ì‹œë“œ
            output_path: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
            
        Returns:
            ìƒì„±ëœ ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        """
        if not self.loaded:
            raise RuntimeError("Model not loaded. Call load_model() first.")
        
        try:
            logger.info(f"ğŸ¬ Generating video from text")
            logger.info(f"   Prompt: {prompt[:100]}...")
            logger.info(f"   Frames: {num_frames}, FPS: {fps}, Size: {width}x{height}")
            
            start_time = time.time()
            
            # ì‹œë“œ ì„¤ì •
            generator = None
            if seed is not None:
                generator = torch.Generator(device=self.device).manual_seed(seed)
            
            # ë„¤ê±°í‹°ë¸Œ í”„ë¡¬í”„íŠ¸
            if negative_prompt is None:
                negative_prompt = (
                    "bad quality, low quality, blurry, distorted, "
                    "deformed, ugly, text, watermark"
                )
            
            # ë¹„ë””ì˜¤ ìƒì„±
            output = self.pipe(
                prompt=prompt,
                negative_prompt=negative_prompt,
                num_frames=num_frames,
                width=width,
                height=height,
                num_inference_steps=num_inference_steps,
                guidance_scale=guidance_scale,
                generator=generator
            )
            
            frames = output.frames[0]  # List of PIL Images
            
            # ì¶œë ¥ ê²½ë¡œ ì„¤ì •
            if output_path is None:
                output_dir = self.models_dir.parent / "output" / "videos"
                output_dir.mkdir(parents=True, exist_ok=True)
                timestamp = int(time.time())
                output_path = output_dir / f"video_{timestamp}.mp4"
            
            # ë¹„ë””ì˜¤ ì €ì¥
            export_to_video(frames, str(output_path), fps=fps)
            
            gen_time = time.time() - start_time
            duration = num_frames / fps
            
            logger.info(f"âœ… Video generated in {gen_time:.1f}s")
            logger.info(f"   Duration: {duration:.1f}s")
            logger.info(f"   Saved: {output_path.name}")
            
            return str(output_path)
            
        except Exception as e:
            logger.error(f"âŒ Video generation failed: {str(e)}")
            raise
    
    def image_to_video(
        self,
        image: Union[str, Path, Image.Image],
        prompt: str,
        negative_prompt: Optional[str] = None,
        num_frames: int = 16,
        fps: int = 8,
        num_inference_steps: int = 25,
        guidance_scale: float = 7.5,
        seed: Optional[int] = None,
        output_path: Optional[Path] = None
    ) -> str:
        """ì´ë¯¸ì§€ë¡œë¶€í„° ë¹„ë””ì˜¤ ìƒì„± (Image-to-Video)
        
        Args:
            image: ì…ë ¥ ì´ë¯¸ì§€ (ê²½ë¡œ ë˜ëŠ” PIL Image)
            prompt: ëª¨ì…˜ í”„ë¡¬í”„íŠ¸
            negative_prompt: ë„¤ê±°í‹°ë¸Œ í”„ë¡¬í”„íŠ¸
            num_frames: í”„ë ˆì„ ìˆ˜
            fps: ì´ˆë‹¹ í”„ë ˆì„
            num_inference_steps: ì¶”ë¡  ìŠ¤í…
            guidance_scale: ê°€ì´ë˜ìŠ¤ ìŠ¤ì¼€ì¼
            seed: ëœë¤ ì‹œë“œ
            output_path: ì¶œë ¥ ê²½ë¡œ
            
        Returns:
            ìƒì„±ëœ ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        """
        if not self.loaded:
            raise RuntimeError("Model not loaded.")
        
        try:
            # ì´ë¯¸ì§€ ë¡œë“œ
            if isinstance(image, (str, Path)):
                image = Image.open(image).convert("RGB")
            
            logger.info(f"ğŸ¬ Generating video from image")
            logger.info(f"   Image size: {image.size}")
            logger.info(f"   Prompt: {prompt[:100]}...")
            
            # Image-to-VideoëŠ” IP-Adapterë‚˜ ControlNet í•„ìš”
            # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ text-to-videoë¡œ êµ¬í˜„
            # ì‹¤ì œë¡œëŠ” img2img íŒŒì´í”„ë¼ì¸ì´ë‚˜ IP-Adapter ì‚¬ìš© ê¶Œì¥
            
            # ì´ë¯¸ì§€ í¬ê¸°ì— ë§ì¶¤
            width, height = image.size
            
            # ë¹„ë””ì˜¤ ìƒì„± (ì´ë¯¸ì§€ ì •ë³´ë¥¼ í”„ë¡¬í”„íŠ¸ì— í†µí•©)
            enhanced_prompt = f"{prompt}, based on the reference image style"
            
            return self.text_to_video(
                prompt=enhanced_prompt,
                negative_prompt=negative_prompt,
                num_frames=num_frames,
                fps=fps,
                width=width,
                height=height,
                num_inference_steps=num_inference_steps,
                guidance_scale=guidance_scale,
                seed=seed,
                output_path=output_path
            )
            
        except Exception as e:
            logger.error(f"âŒ Image-to-video failed: {str(e)}")
            raise
    
    def generate_character_video(
        self,
        character_id: str,
        character_prompt: str,
        action_prompt: str,
        duration_seconds: float = 2.0,
        aspect_ratio: str = "9:16",
        output_path: Optional[Path] = None
    ) -> str:
        """ìºë¦­í„° ë¹„ë””ì˜¤ ìƒì„± (ì‡¼ì¸ ìš©)
        
        Args:
            character_id: ìºë¦­í„° ID
            character_prompt: ìºë¦­í„° ì™¸í˜• í”„ë¡¬í”„íŠ¸
            action_prompt: ì•¡ì…˜/ëª¨ì…˜ í”„ë¡¬í”„íŠ¸
            duration_seconds: ë¹„ë””ì˜¤ ê¸¸ì´ (ì´ˆ)
            aspect_ratio: í™”ë©´ ë¹„ìœ¨ (9:16 ì„¸ë¡œ, 16:9 ê°€ë¡œ)
            output_path: ì¶œë ¥ ê²½ë¡œ
            
        Returns:
            ìƒì„±ëœ ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        """
        try:
            # í™”ë©´ ë¹„ìœ¨ ê³„ì‚°
            if aspect_ratio == "9:16":
                width, height = 512, 896  # ì„¸ë¡œ (ì‡¼ì¸ )
            elif aspect_ratio == "16:9":
                width, height = 896, 512  # ê°€ë¡œ
            else:
                width, height = 512, 512  # ì •ì‚¬ê°í˜•
            
            # FPS ë° í”„ë ˆì„ ìˆ˜ ê³„ì‚°
            fps = 8
            num_frames = int(duration_seconds * fps)
            num_frames = min(max(num_frames, 8), 24)  # 8-24 í”„ë ˆì„ ì œí•œ
            
            # í”„ë¡¬í”„íŠ¸ ê²°í•©
            full_prompt = f"{character_prompt}, {action_prompt}, smooth animation, fluid motion"
            
            logger.info(f"ğŸ¬ Generating character video: {character_id}")
            logger.info(f"   Duration: {duration_seconds}s")
            logger.info(f"   Aspect ratio: {aspect_ratio} ({width}x{height})")
            logger.info(f"   Frames: {num_frames} @ {fps} FPS")
            
            # ë¹„ë””ì˜¤ ìƒì„±
            return self.text_to_video(
                prompt=full_prompt,
                num_frames=num_frames,
                fps=fps,
                width=width,
                height=height,
                output_path=output_path
            )
            
        except Exception as e:
            logger.error(f"âŒ Character video generation failed: {str(e)}")
            raise
    
    def unload_model(self):
        """ëª¨ë¸ ì–¸ë¡œë“œ"""
        if self.pipe is not None:
            del self.pipe
            self.pipe = None
        
        if self.motion_adapter is not None:
            del self.motion_adapter
            self.motion_adapter = None
        
        if self.device == "cuda":
            torch.cuda.empty_cache()
        
        logger.info("ğŸ—‘ï¸ Video generator unloaded")
        self.loaded = False


# ========== ìºë¦­í„° ì•¡ì…˜ í”„ë¡¬í”„íŠ¸ ==========

CHARACTER_ACTIONS = {
    "executive-fox": [
        "confidently presenting with professional gestures",
        "analyzing documents with focused expression",
        "making decisive hand movements while explaining",
        "nodding approvingly with executive presence"
    ],
    "tech-fox": [
        "interacting with holographic displays",
        "typing on futuristic keyboard",
        "demonstrating tech gadgets enthusiastically",
        "explaining with tech hand gestures"
    ],
    "comedian-parrot": [
        "telling jokes with expressive movements",
        "laughing and gesturing humorously",
        "dancing with energetic motions",
        "making funny faces and poses"
    ],
}

def get_character_action(character_id: str, action_type: str = "default") -> str:
    """ìºë¦­í„°ë³„ ì•¡ì…˜ í”„ë¡¬í”„íŠ¸ ê°€ì ¸ì˜¤ê¸°"""
    actions = CHARACTER_ACTIONS.get(character_id, CHARACTER_ACTIONS["executive-fox"])
    return actions[0] if action_type == "default" else actions[0]


# ========== í…ŒìŠ¤íŠ¸ ì½”ë“œ ==========
if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    models_dir = Path(__file__).parent.parent.parent / "models"
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    logger.info(f"ğŸ”§ Device: {device}")
    
    generator = VideoGenerator(models_dir, device)
    
    # ëª¨ë¸ ë¡œë“œ (ìµœì´ˆ 1íšŒ, 5-10ë¶„ ì†Œìš”)
    # generator.load_model()
    
    # ë¹„ë””ì˜¤ ìƒì„± í…ŒìŠ¤íŠ¸
    # prompt = "A cute fox character in business suit, professional animation, Pixar style"
    # video = generator.text_to_video(
    #     prompt=prompt,
    #     num_frames=16,
    #     fps=8
    # )
    # print(f"Generated: {video}")
