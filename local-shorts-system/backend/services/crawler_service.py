#!/usr/bin/env python3
"""
ğŸ•·ï¸ ì›¹ í¬ë¡¤ëŸ¬ ì„œë¹„ìŠ¤
ì œí’ˆ í˜ì´ì§€ì—ì„œ ì •ë³´ ìë™ ì¶”ì¶œ
"""

from playwright.async_api import async_playwright, Page
from bs4 import BeautifulSoup
from pathlib import Path
from typing import Optional, Dict, List
from loguru import logger
import asyncio
import re

class CrawlerService:
    """ì›¹ í¬ë¡¤ë§ ì„œë¹„ìŠ¤"""
    
    def __init__(self, headless: bool = True):
        self.headless = headless
        self.browser = None
        self.playwright = None
    
    async def __aenter__(self):
        """ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§„ì…"""
        self.playwright = await async_playwright().start()
        self.browser = await self.playwright.chromium.launch(headless=self.headless)
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ"""
        if self.browser:
            await self.browser.close()
        if self.playwright:
            await self.playwright.stop()
    
    async def crawl_product(self, url: str) -> Dict:
        """ì œí’ˆ í˜ì´ì§€ í¬ë¡¤ë§
        
        Args:
            url: ì œí’ˆ í˜ì´ì§€ URL
            
        Returns:
            ì œí’ˆ ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        try:
            logger.info(f"ğŸ•·ï¸ Crawling: {url}")
            
            # í˜ì´ì§€ ë¡œë“œ
            page = await self.browser.new_page()
            
            try:
                await page.goto(url, wait_until="networkidle", timeout=30000)
                await asyncio.sleep(2)  # ë™ì  ì½˜í…ì¸  ë¡œë“œ ëŒ€ê¸°
                
                # HTML ê°€ì ¸ì˜¤ê¸°
                html = await page.content()
                
                # ìŠ¤í¬ë¦°ìƒ· (ë””ë²„ê¹…ìš©)
                # await page.screenshot(path="screenshot.png")
                
            finally:
                await page.close()
            
            # BeautifulSoupìœ¼ë¡œ íŒŒì‹±
            soup = BeautifulSoup(html, 'html.parser')
            
            # ì œí’ˆ ì •ë³´ ì¶”ì¶œ
            product_info = self._extract_product_info(soup, url)
            
            logger.info(f"âœ… Crawled: {product_info.get('title', 'Unknown')}")
            
            return product_info
            
        except Exception as e:
            logger.error(f"âŒ Crawling failed: {str(e)}")
            # í´ë°±: ê¸°ë³¸ ì •ë³´ ë°˜í™˜
            return {
                "title": "ì œí’ˆ",
                "description": "í¬ë¡¤ë§ ì‹¤íŒ¨",
                "features": [],
                "price": "ì •ë³´ ì—†ìŒ",
                "url": url,
                "images": []
            }
    
    def _extract_product_info(self, soup: BeautifulSoup, url: str) -> Dict:
        """HTMLì—ì„œ ì œí’ˆ ì •ë³´ ì¶”ì¶œ"""
        
        # URL ê¸°ë°˜ ì‚¬ì´íŠ¸ ê°ì§€
        if "naver.com" in url:
            return self._extract_naver(soup, url)
        elif "coupang.com" in url:
            return self._extract_coupang(soup, url)
        elif "11st.co.kr" in url:
            return self._extract_11st(soup, url)
        else:
            return self._extract_generic(soup, url)
    
    def _extract_naver(self, soup: BeautifulSoup, url: str) -> Dict:
        """ë„¤ì´ë²„ ì‡¼í•‘/ë¸”ë¡œê·¸ í¬ë¡¤ë§"""
        
        # ì œëª©
        title = ""
        title_elem = soup.select_one("h1, .se-title, .post_title, .product_title")
        if title_elem:
            title = title_elem.get_text(strip=True)
        
        # ì„¤ëª…
        description = ""
        desc_elem = soup.select_one(".se-text, .post_ct, .product_description")
        if desc_elem:
            description = desc_elem.get_text(strip=True)[:500]
        
        # ê°€ê²©
        price = ""
        price_elem = soup.select_one(".price, .product_price, span[class*='price']")
        if price_elem:
            price = price_elem.get_text(strip=True)
        
        # ì´ë¯¸ì§€
        images = []
        img_elems = soup.select("img[src*='http']")
        for img in img_elems[:5]:  # ìµœëŒ€ 5ê°œ
            src = img.get("src")
            if src and any(ext in src for ext in [".jpg", ".jpeg", ".png"]):
                images.append(src)
        
        # íŠ¹ì§• ì¶”ì¶œ (ë¦¬ìŠ¤íŠ¸)
        features = []
        list_elems = soup.select("ul li, ol li")
        for li in list_elems[:5]:
            text = li.get_text(strip=True)
            if text and len(text) < 100:
                features.append(text)
        
        return {
            "title": title or "ì œí’ˆ",
            "description": description or "ì„¤ëª… ì—†ìŒ",
            "features": features,
            "price": price,
            "url": url,
            "images": images,
            "source": "naver"
        }
    
    def _extract_coupang(self, soup: BeautifulSoup, url: str) -> Dict:
        """ì¿ íŒ¡ í¬ë¡¤ë§"""
        
        # ì œëª©
        title = ""
        title_elem = soup.select_one(".prod-buy-header__title, h1")
        if title_elem:
            title = title_elem.get_text(strip=True)
        
        # ê°€ê²©
        price = ""
        price_elem = soup.select_one(".total-price strong, .price-value")
        if price_elem:
            price = price_elem.get_text(strip=True) + "ì›"
        
        # ì´ë¯¸ì§€
        images = []
        img_elems = soup.select("img.prod-image__detail")
        for img in img_elems[:5]:
            src = img.get("src") or img.get("data-src")
            if src:
                images.append(src)
        
        # íŠ¹ì§•
        features = []
        feature_elems = soup.select(".prod-description__attribute li")
        for elem in feature_elems[:5]:
            features.append(elem.get_text(strip=True))
        
        return {
            "title": title or "ì œí’ˆ",
            "description": "",
            "features": features,
            "price": price,
            "url": url,
            "images": images,
            "source": "coupang"
        }
    
    def _extract_11st(self, soup: BeautifulSoup, url: str) -> Dict:
        """11ë²ˆê°€ í¬ë¡¤ë§"""
        
        title = ""
        title_elem = soup.select_one(".title, h1")
        if title_elem:
            title = title_elem.get_text(strip=True)
        
        price = ""
        price_elem = soup.select_one(".price, .sale_price")
        if price_elem:
            price = price_elem.get_text(strip=True)
        
        images = []
        img_elems = soup.select("img[src*='product']")
        for img in img_elems[:5]:
            src = img.get("src")
            if src:
                images.append(src)
        
        return {
            "title": title or "ì œí’ˆ",
            "description": "",
            "features": [],
            "price": price,
            "url": url,
            "images": images,
            "source": "11st"
        }
    
    def _extract_generic(self, soup: BeautifulSoup, url: str) -> Dict:
        """ì¼ë°˜ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ (íœ´ë¦¬ìŠ¤í‹±)"""
        
        # ì œëª© ì¶”ì¶œ ì‹œë„
        title = ""
        for selector in ["h1", ".product-title", ".title", "[itemprop='name']"]:
            elem = soup.select_one(selector)
            if elem:
                title = elem.get_text(strip=True)
                break
        
        if not title:
            title_tag = soup.find("title")
            if title_tag:
                title = title_tag.get_text(strip=True)
        
        # ì„¤ëª…
        description = ""
        for selector in ["meta[name='description']", ".description", ".product-description"]:
            elem = soup.select_one(selector)
            if elem:
                description = elem.get("content", "") or elem.get_text(strip=True)
                break
        
        # ê°€ê²©
        price = ""
        for selector in [".price", "[itemprop='price']", "span[class*='price']"]:
            elem = soup.select_one(selector)
            if elem:
                price = elem.get_text(strip=True)
                break
        
        # ì´ë¯¸ì§€
        images = []
        img_elems = soup.select("img[src*='http']")
        for img in img_elems[:5]:
            src = img.get("src")
            if src and any(ext in src for ext in [".jpg", ".jpeg", ".png"]):
                images.append(src)
        
        # íŠ¹ì§• (ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œ)
        features = []
        list_items = soup.select("ul li, ol li")
        for li in list_items[:5]:
            text = li.get_text(strip=True)
            if text and 10 < len(text) < 100:
                features.append(text)
        
        return {
            "title": title or "ì œí’ˆ",
            "description": description[:500] if description else "",
            "features": features,
            "price": price,
            "url": url,
            "images": images,
            "source": "generic"
        }
    
    @staticmethod
    def clean_text(text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ë¦¬"""
        # ì—°ì† ê³µë°± ì œê±°
        text = re.sub(r'\s+', ' ', text)
        # íŠ¹ìˆ˜ë¬¸ì ì œê±° (í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê¸°ë³¸ ë¬¸ì¥ë¶€í˜¸ë§Œ)
        text = re.sub(r'[^\w\s\.\,\!\?\(\)\-\+\/\:]', '', text)
        return text.strip()


# ========== ê°„ë‹¨í•œ ë™ê¸° ë˜í¼ ==========
class SimpleCrawler:
    """ê°„ë‹¨í•œ ë™ê¸° í¬ë¡¤ëŸ¬ (Playwright ì—†ì´)"""
    
    @staticmethod
    def crawl_simple(url: str) -> Dict:
        """requests + BeautifulSoup ì‚¬ìš© (ë¹ ë¦„)"""
        import requests
        
        try:
            logger.info(f"ğŸ•·ï¸ Simple crawling: {url}")
            
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            }
            
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ì œëª©
            title = ""
            h1 = soup.find("h1")
            if h1:
                title = h1.get_text(strip=True)
            else:
                title_tag = soup.find("title")
                if title_tag:
                    title = title_tag.get_text(strip=True)
            
            # ì„¤ëª…
            meta_desc = soup.find("meta", {"name": "description"})
            description = meta_desc.get("content", "") if meta_desc else ""
            
            # ì´ë¯¸ì§€
            images = []
            for img in soup.find_all("img", limit=5):
                src = img.get("src")
                if src and "http" in src:
                    images.append(src)
            
            logger.info(f"âœ… Simple crawl: {title}")
            
            return {
                "title": title or "ì œí’ˆ",
                "description": description,
                "features": [],
                "price": "",
                "url": url,
                "images": images,
                "source": "simple"
            }
            
        except Exception as e:
            logger.error(f"âŒ Simple crawl failed: {str(e)}")
            return {
                "title": "ì œí’ˆ",
                "description": "í¬ë¡¤ë§ ì‹¤íŒ¨",
                "features": [],
                "price": "",
                "url": url,
                "images": []
            }


# ========== í…ŒìŠ¤íŠ¸ ì½”ë“œ ==========
if __name__ == "__main__":
    import asyncio
    
    async def test_crawl():
        test_url = "https://blog.naver.com/example"
        
        # Playwright í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸
        async with CrawlerService(headless=True) as crawler:
            result = await crawler.crawl_product(test_url)
            print(json.dumps(result, indent=2, ensure_ascii=False))
    
    # asyncio.run(test_crawl())
    
    # Simple í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸
    result = SimpleCrawler.crawl_simple("https://www.example.com")
    print(result)
