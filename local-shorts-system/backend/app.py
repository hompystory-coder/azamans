#!/usr/bin/env python3
"""
ğŸ¬ ë¡œì»¬ PC AI ì‡¼ì¸  ìƒì„± ì‹œìŠ¤í…œ
FastAPI ë°±ì—”ë“œ ì„œë²„
"""

import os
import sys
from pathlib import Path
from typing import Optional
import uvicorn
from fastapi import FastAPI, HTTPException, BackgroundTasks, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, FileResponse
from pydantic import BaseModel
from loguru import logger
import torch

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ
BASE_DIR = Path(__file__).parent.parent
sys.path.insert(0, str(BASE_DIR))

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
from dotenv import load_dotenv
load_dotenv()

# ========== FastAPI ì•± ìƒì„± ==========
app = FastAPI(
    title="Local AI Shorts Generator",
    description="ì‚¬ìš©ì PC ë¦¬ì†ŒìŠ¤ë¥¼ í™œìš©í•œ ë¡œì»¬ AI ì‡¼ì¸  ìƒì„± API",
    version="0.1.0"
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # í”„ë¡œë•ì…˜ì—ì„œëŠ” ì œí•œ í•„ìš”
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ========== ì„œë¹„ìŠ¤ ì„í¬íŠ¸ ==========
from services.pipeline_service import PipelineService
from services.crawler_service import SimpleCrawler
import asyncio
import uuid

# ========== ì „ì—­ ë³€ìˆ˜ ==========
OUTPUT_DIR = BASE_DIR / "output"
MODELS_DIR = BASE_DIR / "models"
VIDEOS_DIR = OUTPUT_DIR / "videos"
AUDIO_DIR = OUTPUT_DIR / "audio"
TEMP_DIR = OUTPUT_DIR / "temp"

# ë””ë ‰í† ë¦¬ ìƒì„±
for dir_path in [OUTPUT_DIR, MODELS_DIR, VIDEOS_DIR, AUDIO_DIR, TEMP_DIR]:
    dir_path.mkdir(parents=True, exist_ok=True)

# GPU í™•ì¸
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
logger.info(f"ğŸ”§ Device: {DEVICE}")
if DEVICE == "cuda":
    logger.info(f"ğŸ® GPU: {torch.cuda.get_device_name(0)}")
    logger.info(f"ğŸ’¾ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")

# íŒŒì´í”„ë¼ì¸ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
pipeline = PipelineService(MODELS_DIR, OUTPUT_DIR, DEVICE)

# ========== Pydantic ëª¨ë¸ ==========
class ShortsGenerationRequest(BaseModel):
    """ì‡¼ì¸  ìƒì„± ìš”ì²­"""
    url: Optional[str] = None
    text_content: Optional[str] = None
    character_id: str = "executive-fox"
    video_mode: str = "character_plus_images"
    category: str = "general"
    duration: int = 15  # ì´ˆ
    aspect_ratio: str = "9:16"
    
class SystemInfoResponse(BaseModel):
    """ì‹œìŠ¤í…œ ì •ë³´"""
    device: str
    gpu_name: Optional[str] = None
    vram_gb: Optional[float] = None
    models_downloaded: bool = False
    status: str = "ready"

class JobStatusResponse(BaseModel):
    """ì‘ì—… ìƒíƒœ"""
    job_id: str
    status: str  # pending, processing, completed, failed
    progress: int  # 0-100
    message: str
    output_path: Optional[str] = None
    error: Optional[str] = None

# ========== API ì—”ë“œí¬ì¸íŠ¸ ==========

@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "name": "Local AI Shorts Generator",
        "version": "0.1.0",
        "status": "running",
        "device": DEVICE
    }

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    return {
        "status": "healthy",
        "device": DEVICE,
        "timestamp": str(Path(__file__).stat().st_mtime)
    }

@app.get("/api/system/info", response_model=SystemInfoResponse)
async def get_system_info():
    """ì‹œìŠ¤í…œ ì •ë³´ ì¡°íšŒ"""
    info = {
        "device": DEVICE,
        "status": "ready"
    }
    
    if DEVICE == "cuda":
        info["gpu_name"] = torch.cuda.get_device_name(0)
        info["vram_gb"] = round(torch.cuda.get_device_properties(0).total_memory / 1024**3, 1)
    
    # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ í™•ì¸
    sd_path = MODELS_DIR / "stable-diffusion-xl-base-1.0"
    tts_path = MODELS_DIR / "xtts-v2"
    info["models_downloaded"] = sd_path.exists() or tts_path.exists()
    
    return info

@app.get("/api/characters")
async def get_characters():
    """39ê°œ ìºë¦­í„° ëª©ë¡"""
    characters = {
        "business": [
            {"id": "executive-fox", "name": "ğŸ¦Š ì´ê·¸ì œíí‹°ë¸Œ í­ìŠ¤", "category": "business"},
            {"id": "ceo-lion", "name": "ğŸ¦ CEO ë¼ì´ì˜¨", "category": "business"},
            {"id": "strategist-eagle", "name": "ğŸ¦… ì „ëµê°€ ì´ê¸€", "category": "business"},
            {"id": "negotiator-wolf", "name": "ğŸº í˜‘ìƒê°€ ìš¸í”„", "category": "business"},
            {"id": "consultant-owl", "name": "ğŸ¦‰ ì»¨ì„¤í„´íŠ¸ ì•„ìš¸", "category": "business"},
        ],
        "tech": [
            {"id": "tech-fox", "name": "ğŸ¦Š í…Œí¬ í­ìŠ¤", "category": "tech"},
            {"id": "dev-raccoon", "name": "ğŸ¦ ê°œë°œì ë¼ì¿¤", "category": "tech"},
            {"id": "ai-panda", "name": "ğŸ¼ AI íŒë‹¤", "category": "tech"},
            {"id": "startup-tiger", "name": "ğŸ¯ ìŠ¤íƒ€íŠ¸ì—… íƒ€ì´ê±°", "category": "tech"},
            {"id": "blockchain-monkey", "name": "ğŸµ ë¸”ë¡ì²´ì¸ ëª½í‚¤", "category": "tech"},
        ],
        "fashion": [
            {"id": "fashionista-cat", "name": "ğŸ˜º íŒ¨ì…”ë‹ˆìŠ¤íƒ€ ìº£", "category": "fashion"},
            {"id": "stylist-peacock", "name": "ğŸ¦š ìŠ¤íƒ€ì¼ë¦¬ìŠ¤íŠ¸ í”¼ì½•", "category": "fashion"},
            {"id": "luxury-leopard", "name": "ğŸ† ëŸ­ì…”ë¦¬ ë ˆì˜¤íŒŒë“œ", "category": "fashion"},
            {"id": "trendy-rabbit", "name": "ğŸ° íŠ¸ë Œë”” ë˜ë¹—", "category": "fashion"},
            {"id": "designer-swan", "name": "ğŸ¦¢ ë””ìì´ë„ˆ ìŠ¤ì™„", "category": "fashion"},
        ],
        "sports": [
            {"id": "athlete-cheetah", "name": "ğŸ† ì• ìŠ¬ë¦¬íŠ¸ ì¹˜íƒ€", "category": "sports"},
            {"id": "trainer-bear", "name": "ğŸ» íŠ¸ë ˆì´ë„ˆ ë² ì–´", "category": "sports"},
            {"id": "yoga-deer", "name": "ğŸ¦Œ ìš”ê°€ ë””ì–´", "category": "sports"},
            {"id": "runner-kangaroo", "name": "ğŸ¦˜ ëŸ¬ë„ˆ ìº¥ê±°ë£¨", "category": "sports"},
            {"id": "fighter-dragon", "name": "ğŸ‰ íŒŒì´í„° ë“œë˜ê³¤", "category": "sports"},
        ],
        "food": [
            {"id": "chef-penguin", "name": "ğŸ§ ì…°í”„ í­ê·„", "category": "food"},
            {"id": "foodie-hamster", "name": "ğŸ¹ í‘¸ë”” í–„ìŠ¤í„°", "category": "food"},
            {"id": "barista-otter", "name": "ğŸ¦¦ ë°”ë¦¬ìŠ¤íƒ€ ì˜¤í„°", "category": "food"},
            {"id": "sommelier-fox", "name": "ğŸ¦Š ì†Œë¯ˆë¦¬ì— í­ìŠ¤", "category": "food"},
            {"id": "baker-bear", "name": "ğŸ» ë² ì´ì»¤ ë² ì–´", "category": "food"},
        ],
        "entertainment": [
            {"id": "comedian-parrot", "name": "ğŸ¦œ ì½”ë¯¸ë””ì–¸ íŒ¨ëŸ¿", "category": "entertainment"},
            {"id": "musician-fox", "name": "ğŸ¦Š ë®¤ì§€ì…˜ í­ìŠ¤", "category": "entertainment"},
            {"id": "dancer-peacock", "name": "ğŸ¦š ëŒ„ì„œ í”¼ì½•", "category": "entertainment"},
            {"id": "artist-cat", "name": "ğŸ˜º ì•„í‹°ìŠ¤íŠ¸ ìº£", "category": "entertainment"},
            {"id": "gamer-otter", "name": "ğŸ¦¦ ê²Œì´ë¨¸ ì˜¤í„°", "category": "entertainment"},
        ]
    }
    
    return {
        "total": sum(len(chars) for chars in characters.values()),
        "categories": characters
    }

@app.post("/api/shorts/generate")
async def generate_shorts(
    request: ShortsGenerationRequest,
    background_tasks: BackgroundTasks
):
    """ì‡¼ì¸  ìƒì„± ì‹œì‘ (ì™„ì „ ìë™í™”)"""
    try:
        # Job ID ìƒì„±
        import time
        job_id = f"shorts_{int(time.time())}_{uuid.uuid4().hex[:6]}"
        
        logger.info(f"ğŸ¬ New shorts generation job: {job_id}")
        logger.info(f"   Character: {request.character_id}")
        logger.info(f"   URL: {request.url}")
        
        # ìš”ì²­ ë°ì´í„° ì¤€ë¹„
        request_data = {
            "character_id": request.character_id,
            "num_scenes": 5,
            "duration": request.duration,
            "aspect_ratio": request.aspect_ratio
        }
        
        # URLì´ ìˆìœ¼ë©´ í¬ë¡¤ë§
        if request.url:
            request_data["url"] = request.url
        elif request.text_content:
            # í…ìŠ¤íŠ¸ ì§ì ‘ ì œê³µ
            request_data["product_info"] = {
                "title": "ì œí’ˆ",
                "description": request.text_content,
                "features": [],
                "price": ""
            }
        else:
            # ê¸°ë³¸ ì •ë³´
            request_data["product_info"] = {
                "title": "í”„ë¦¬ë¯¸ì—„ ì œí’ˆ",
                "description": "ìµœê³ ì˜ í’ˆì§ˆ",
                "features": ["ê³ í’ˆì§ˆ", "í•©ë¦¬ì ì¸ ê°€ê²©"],
                "price": "99,000ì›"
            }
        
        # ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…ìœ¼ë¡œ ì‡¼ì¸  ìƒì„±
        background_tasks.add_task(
            _generate_shorts_background,
            job_id,
            request_data
        )
        
        return {
            "job_id": job_id,
            "status": "pending",
            "message": "ì‡¼ì¸  ìƒì„±ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. 5-10ë¶„ ì†Œìš”ë©ë‹ˆë‹¤.",
            "estimated_time": "5-10ë¶„"
        }
        
    except Exception as e:
        logger.error(f"âŒ Error starting shorts generation: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


async def _generate_shorts_background(job_id: str, request_data: dict):
    """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‡¼ì¸  ìƒì„±"""
    try:
        result = await pipeline.generate_shorts(job_id, request_data)
        logger.info(f"âœ… Background generation completed: {job_id}")
    except Exception as e:
        logger.error(f"âŒ Background generation failed: {str(e)}")

@app.get("/api/shorts/status/{job_id}", response_model=JobStatusResponse)
async def get_job_status(job_id: str):
    """ì‘ì—… ìƒíƒœ ì¡°íšŒ"""
    job_status = pipeline.get_job_status(job_id)
    
    if not job_status:
        raise HTTPException(status_code=404, detail="Job not found")
    
    return {
        "job_id": job_id,
        "status": job_status["status"],
        "progress": job_status["progress"],
        "message": job_status["message"],
        "output_path": job_status.get("output_path"),
        "error": job_status.get("error")
    }

@app.get("/api/shorts/download/{job_id}")
async def download_shorts(job_id: str):
    """ì™„ì„±ëœ ì‡¼ì¸  ë‹¤ìš´ë¡œë“œ"""
    video_path = VIDEOS_DIR / f"{job_id}.mp4"
    
    if not video_path.exists():
        raise HTTPException(status_code=404, detail="Video not found")
    
    return FileResponse(
        video_path,
        media_type="video/mp4",
        filename=f"{job_id}.mp4"
    )

@app.post("/api/models/install")
async def install_models(background_tasks: BackgroundTasks):
    """AI ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ì„¤ì¹˜"""
    try:
        # ë°±ê·¸ë¼ìš´ë“œë¡œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
        # background_tasks.add_task(download_models)
        
        return {
            "status": "started",
            "message": "ëª¨ë¸ ë‹¤ìš´ë¡œë“œê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. ìˆ˜ ë¶„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
            "models": [
                "Stable Diffusion XL",
                "AnimateDiff",
                "Coqui TTS XTTS-v2",
                "LLaMA 3.1 8B (Ollama ë³„ë„)"
            ]
        }
    except Exception as e:
        logger.error(f"âŒ Error installing models: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/models/status")
async def get_models_status():
    """ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ìƒíƒœ"""
    models_status = {}
    
    # Stable Diffusion í™•ì¸
    sd_path = MODELS_DIR / "stable-diffusion-xl-base-1.0"
    models_status["stable_diffusion"] = {
        "name": "Stable Diffusion XL",
        "downloaded": sd_path.exists(),
        "size_gb": 6.9
    }
    
    # AnimateDiff í™•ì¸
    ad_path = MODELS_DIR / "animatediff"
    models_status["animatediff"] = {
        "name": "AnimateDiff",
        "downloaded": ad_path.exists(),
        "size_gb": 1.7
    }
    
    # TTS í™•ì¸
    tts_path = MODELS_DIR / "xtts-v2"
    models_status["tts"] = {
        "name": "Coqui TTS XTTS-v2",
        "downloaded": tts_path.exists(),
        "size_gb": 2.0
    }
    
    return {
        "models": models_status,
        "total_size_gb": sum(m["size_gb"] for m in models_status.values()),
        "all_downloaded": all(m["downloaded"] for m in models_status.values())
    }

# ========== ë©”ì¸ ì‹¤í–‰ ==========
if __name__ == "__main__":
    logger.info("ğŸš€ Starting Local AI Shorts Generator Backend")
    logger.info(f"ğŸ“ Output directory: {OUTPUT_DIR}")
    logger.info(f"ğŸ¤– Models directory: {MODELS_DIR}")
    
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        log_level="info"
    )
