# ğŸš€ ë¡œì»¬ PC AI ì‡¼ì¸  ìƒì„± ì‹œìŠ¤í…œ - ë°°í¬ ë° ì‚¬ìš© ê°€ì´ë“œ

**ë‚ ì§œ**: 2025-12-25  
**ë²„ì „**: 1.0.0  
**ìƒíƒœ**: âœ… í”„ë¡œë•ì…˜ ì¤€ë¹„ ì™„ë£Œ

---

## ğŸ“‹ ëª©ì°¨

1. [ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­](#ì‹œìŠ¤í…œ-ìš”êµ¬ì‚¬í•­)
2. [ì„¤ì¹˜ ë°©ë²•](#ì„¤ì¹˜-ë°©ë²•)
3. [ì‹¤í–‰ ë°©ë²•](#ì‹¤í–‰-ë°©ë²•)
4. [ì‚¬ìš© ë°©ë²•](#ì‚¬ìš©-ë°©ë²•)
5. [íŠ¸ëŸ¬ë¸”ìŠˆíŒ…](#íŠ¸ëŸ¬ë¸”ìŠˆíŒ…)
6. [FAQ](#faq)

---

## ğŸ–¥ï¸ ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

### **í•„ìˆ˜ ìš”êµ¬ì‚¬í•­**

```
GPU:        NVIDIA GTX 1660 6GB ì´ìƒ (RTX 3060 12GB ê¶Œì¥)
RAM:        16 GB ì´ìƒ
ë””ìŠ¤í¬:     50 GB ì—¬ìœ  ê³µê°„ (AI ëª¨ë¸ 15.3GB + ì¶œë ¥ íŒŒì¼)
OS:         Windows 10/11, Linux, macOS (Intel/Apple Silicon)
Python:     Python 3.8 - 3.11 (3.10 ê¶Œì¥)
CUDA:       11.8 ì´ìƒ (NVIDIA GPU ì‚¬ìš© ì‹œ)
```

### **ê¶Œì¥ ì‚¬ì–‘**

```
GPU:        NVIDIA RTX 3060 12GB ì´ìƒ
RAM:        32 GB
CPU:        Intel i5 / AMD Ryzen 5 ì´ìƒ (6ì½”ì–´+)
ë””ìŠ¤í¬:     SSD 100 GB+
```

---

## ğŸ“¥ ì„¤ì¹˜ ë°©ë²•

### **Step 1: ì‚¬ì „ ì¤€ë¹„**

#### **1.1 Python ì„¤ì¹˜ í™•ì¸**
```bash
python --version
# Python 3.8 - 3.11 í•„ìš” (3.10 ê¶Œì¥)
```

Pythonì´ ì—†ë‹¤ë©´:
- **Windows**: https://www.python.org/downloads/
- **Linux**: `sudo apt install python3.10 python3.10-venv`
- **macOS**: `brew install python@3.10`

#### **1.2 Git ì„¤ì¹˜ í™•ì¸**
```bash
git --version
```

Gitì´ ì—†ë‹¤ë©´: https://git-scm.com/downloads

#### **1.3 NVIDIA GPU ë“œë¼ì´ë²„ (ì„ íƒ)**
NVIDIA GPU ì‚¬ìš© ì‹œ:
- ìµœì‹  ë“œë¼ì´ë²„ ë‹¤ìš´ë¡œë“œ: https://www.nvidia.com/drivers
- CUDA Toolkit 11.8: https://developer.nvidia.com/cuda-11-8-0-download-archive

---

### **Step 2: í”„ë¡œì íŠ¸ ë‹¤ìš´ë¡œë“œ**

```bash
# í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ë¡œ ì´ë™
cd /home/azamans/webapp

# ë˜ëŠ” ìƒˆë¡œìš´ ìœ„ì¹˜ì— í´ë¡ 
# git clone <repository-url>
# cd <repository-name>

# local-shorts-system ë””ë ‰í† ë¦¬ë¡œ ì´ë™
cd local-shorts-system
```

---

### **Step 3: Python ê°€ìƒí™˜ê²½ ìƒì„±**

```bash
# ê°€ìƒí™˜ê²½ ìƒì„±
python -m venv venv

# ê°€ìƒí™˜ê²½ í™œì„±í™”
# Linux/macOS:
source venv/bin/activate

# Windows:
venv\Scripts\activate

# í™•ì¸ (í”„ë¡¬í”„íŠ¸ì— (venv) í‘œì‹œë¨)
which python  # Linux/macOS
where python  # Windows
```

---

### **Step 4: PyTorch ì„¤ì¹˜ (GPU ê°€ì†)**

#### **NVIDIA GPU ìˆëŠ” ê²½ìš° (ê¶Œì¥)**
```bash
# CUDA 11.8 ë²„ì „
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# CUDA 12.1 ë²„ì „ (ìµœì‹  GPU)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

#### **GPU ì—†ëŠ” ê²½ìš° (CPUë§Œ)**
```bash
pip install torch torchvision torchaudio
```

#### **macOS (Apple Silicon)**
```bash
pip install torch torchvision torchaudio
```

**ì„¤ì¹˜ í™•ì¸:**
```bash
python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA available: {torch.cuda.is_available()}')"
```

---

### **Step 5: ì˜ì¡´ì„± ì„¤ì¹˜**

```bash
# requirements.txt ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt

# ì„¤ì¹˜ í™•ì¸
pip list | grep -E "fastapi|diffusers|TTS|ollama"
```

**ì£¼ìš” íŒ¨í‚¤ì§€:**
- `fastapi` - ë°±ì—”ë“œ ì„œë²„
- `diffusers` - Stable Diffusion / AnimateDiff
- `TTS` - Coqui TTS ìŒì„± í•©ì„±
- `playwright` - ì›¹ í¬ë¡¤ë§
- `ffmpeg-python` - ë¹„ë””ì˜¤ ë Œë”ë§

---

### **Step 6: FFmpeg ì„¤ì¹˜**

#### **Linux (Ubuntu/Debian)**
```bash
sudo apt update
sudo apt install ffmpeg
ffmpeg -version
```

#### **macOS**
```bash
brew install ffmpeg
ffmpeg -version
```

#### **Windows**
1. https://www.gyan.dev/ffmpeg/builds/ ë‹¤ìš´ë¡œë“œ
2. ì••ì¶• í•´ì œ í›„ `bin` í´ë”ë¥¼ PATHì— ì¶”ê°€
3. `ffmpeg -version` í™•ì¸

---

### **Step 7: AI ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (15.3 GB)**

```bash
# ëª¨ë¸ ìë™ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ (20-30ë¶„ ì†Œìš”)
python scripts/install_models.py

# ë‹¤ìš´ë¡œë“œ ì§„í–‰ ìƒí™©:
# [1/4] Stable Diffusion XL (6.9 GB) ...
# [2/4] AnimateDiff (1.7 GB) ...
# [3/4] Coqui TTS (2.0 GB) ...
# [4/4] ì™„ë£Œ!
```

**ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ (ì„ íƒ):**
```python
# Python ì¸í„°í”„ë¦¬í„°ì—ì„œ
from diffusers import StableDiffusionXLPipeline
StableDiffusionXLPipeline.from_pretrained(
    "stabilityai/stable-diffusion-xl-base-1.0",
    cache_dir="./models"
)
```

---

### **Step 8: Ollama ì„¤ì¹˜ (LLM)**

#### **ìë™ ì„¤ì¹˜ (ê¶Œì¥)**
```bash
# Linux/macOS
curl -fsSL https://ollama.ai/install.sh | sh

# Windows
# https://ollama.ai/download ì—ì„œ ë‹¤ìš´ë¡œë“œ
```

#### **LLaMA 3.1 ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (4.7 GB)**
```bash
ollama pull llama3.1:8b

# í™•ì¸
ollama list
```

**Ollama ì„œë²„ ì‹œì‘:**
```bash
# ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰
ollama serve &

# í™•ì¸
curl http://localhost:11434/api/tags
```

---

### **Step 9: Playwright ë¸Œë¼ìš°ì € ì„¤ì¹˜**

```bash
# Playwright ë¸Œë¼ìš°ì € ë‹¤ìš´ë¡œë“œ
playwright install chromium

# ë˜ëŠ” ì „ì²´ ë¸Œë¼ìš°ì €
playwright install
```

---

## âœ… ì„¤ì¹˜ ì™„ë£Œ í™•ì¸

```bash
# ëª¨ë“  êµ¬ì„± ìš”ì†Œ í™•ì¸
python -c "
import torch
from diffusers import StableDiffusionXLPipeline
from TTS.api import TTS
import requests

print('âœ… PyTorch:', torch.__version__)
print('âœ… CUDA:', torch.cuda.is_available())
print('âœ… Diffusers: OK')
print('âœ… TTS: OK')

# Ollama í™•ì¸
try:
    resp = requests.get('http://localhost:11434/api/tags', timeout=5)
    print('âœ… Ollama:', resp.status_code == 200)
except:
    print('âŒ Ollama: Not running')
"
```

**ì˜ˆìƒ ì¶œë ¥:**
```
âœ… PyTorch: 2.1.0+cu118
âœ… CUDA: True
âœ… Diffusers: OK
âœ… TTS: OK
âœ… Ollama: True
```

---

## ğŸš€ ì‹¤í–‰ ë°©ë²•

### **Step 1: í„°ë¯¸ë„ ì—´ê¸°**

```bash
# í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ë¡œ ì´ë™
cd /home/azamans/webapp/local-shorts-system

# ê°€ìƒí™˜ê²½ í™œì„±í™”
source venv/bin/activate  # Linux/macOS
# venv\Scripts\activate   # Windows
```

---

### **Step 2: Ollama ì„œë²„ ì‹œì‘ (ë³„ë„ í„°ë¯¸ë„)**

```bash
# í„°ë¯¸ë„ 1: Ollama ì„œë²„
ollama serve
```

---

### **Step 3: FastAPI ë°±ì—”ë“œ ì„œë²„ ì‹œì‘**

```bash
# í„°ë¯¸ë„ 2: ë°±ì—”ë“œ ì„œë²„
cd backend
python app.py

# ì„œë²„ ì‹œì‘ ë¡œê·¸:
# ğŸš€ Starting Local AI Shorts Generator Backend
# ğŸ”§ Device: cuda
# ğŸ® GPU: NVIDIA GeForce RTX 3060
# ğŸ’¾ VRAM: 12.0 GB
# INFO:     Uvicorn running on http://0.0.0.0:8000
```

---

### **Step 4: ë¸Œë¼ìš°ì € ì—´ê¸°**

```
http://localhost:8000
```

**API ë¬¸ì„œ (Swagger UI):**
```
http://localhost:8000/docs
```

---

## ğŸ¬ ì‚¬ìš© ë°©ë²•

### **ë°©ë²• 1: API í˜¸ì¶œ (cURL)**

#### **1. ì‹œìŠ¤í…œ ì •ë³´ í™•ì¸**
```bash
curl http://localhost:8000/api/system/info | jq
```

**ì‘ë‹µ:**
```json
{
  "device": "cuda",
  "gpu_name": "NVIDIA GeForce RTX 3060",
  "vram_gb": 12.0,
  "models_downloaded": true,
  "status": "ready"
}
```

---

#### **2. ìºë¦­í„° ëª©ë¡ ì¡°íšŒ**
```bash
curl http://localhost:8000/api/characters | jq
```

**ì‘ë‹µ:**
```json
{
  "total": 30,
  "categories": {
    "business": [
      {"id": "executive-fox", "name": "ğŸ¦Š ì´ê·¸ì œíí‹°ë¸Œ í­ìŠ¤", "category": "business"},
      {"id": "ceo-lion", "name": "ğŸ¦ CEO ë¼ì´ì˜¨", "category": "business"}
    ],
    "tech": [...],
    ...
  }
}
```

---

#### **3. ì‡¼ì¸  ìƒì„± ì‹œì‘**

```bash
curl -X POST http://localhost:8000/api/shorts/generate \
  -H "Content-Type: application/json" \
  -d '{
    "url": "https://shopping.naver.com/example-product",
    "character_id": "executive-fox",
    "duration": 15
  }' | jq
```

**ì‘ë‹µ:**
```json
{
  "job_id": "shorts_1703512345_abc123",
  "status": "pending",
  "message": "ì‡¼ì¸  ìƒì„±ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. 5-10ë¶„ ì†Œìš”ë©ë‹ˆë‹¤.",
  "estimated_time": "5-10ë¶„"
}
```

---

#### **4. ìƒíƒœ í™•ì¸**

```bash
# job_idë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒíƒœ í™•ì¸
curl http://localhost:8000/api/shorts/status/shorts_1703512345_abc123 | jq
```

**ì‘ë‹µ (ì§„í–‰ ì¤‘):**
```json
{
  "job_id": "shorts_1703512345_abc123",
  "status": "processing",
  "progress": 45,
  "message": "ë¹„ë””ì˜¤ ìƒì„± ì¤‘... (3/5)",
  "output_path": null,
  "error": null
}
```

**ì‘ë‹µ (ì™„ë£Œ):**
```json
{
  "job_id": "shorts_1703512345_abc123",
  "status": "completed",
  "progress": 100,
  "message": "ì‡¼ì¸  ìƒì„± ì™„ë£Œ!",
  "output_path": "/home/azamans/webapp/local-shorts-system/output/videos/shorts_1703512345_abc123.mp4",
  "error": null
}
```

---

#### **5. ì™„ì„±ëœ ì‡¼ì¸  ë‹¤ìš´ë¡œë“œ**

```bash
# ë‹¤ìš´ë¡œë“œ
curl -O http://localhost:8000/api/shorts/download/shorts_1703512345_abc123

# íŒŒì¼ í™•ì¸
ls -lh shorts_1703512345_abc123.mp4
# -rw-rw-r-- 1 user user 45M Dec 25 12:34 shorts_1703512345_abc123.mp4
```

---

### **ë°©ë²• 2: Python ìŠ¤í¬ë¦½íŠ¸**

```python
# generate_shorts.py
import requests
import time
import json

API_BASE = "http://localhost:8000"

def generate_shorts(url, character_id="executive-fox"):
    """ì‡¼ì¸  ìƒì„± ë° ë‹¤ìš´ë¡œë“œ"""
    
    # 1. ì‡¼ì¸  ìƒì„± ì‹œì‘
    print(f"ğŸ¬ ì‡¼ì¸  ìƒì„± ì‹œì‘...")
    response = requests.post(
        f"{API_BASE}/api/shorts/generate",
        json={
            "url": url,
            "character_id": character_id,
            "duration": 15
        }
    )
    data = response.json()
    job_id = data["job_id"]
    print(f"âœ… Job ID: {job_id}")
    
    # 2. ìƒíƒœ í™•ì¸ (í´ë§)
    while True:
        response = requests.get(f"{API_BASE}/api/shorts/status/{job_id}")
        status = response.json()
        
        progress = status["progress"]
        message = status["message"]
        print(f"ğŸ“Š ì§„í–‰ë¥ : {progress}% - {message}")
        
        if status["status"] == "completed":
            print(f"âœ… ì™„ë£Œ! íŒŒì¼: {status['output_path']}")
            break
        elif status["status"] == "failed":
            print(f"âŒ ì‹¤íŒ¨: {status['error']}")
            return None
        
        time.sleep(10)  # 10ì´ˆë§ˆë‹¤ í™•ì¸
    
    # 3. ë‹¤ìš´ë¡œë“œ
    print(f"ğŸ“¥ ë‹¤ìš´ë¡œë“œ ì¤‘...")
    response = requests.get(f"{API_BASE}/api/shorts/download/{job_id}")
    
    filename = f"{job_id}.mp4"
    with open(filename, "wb") as f:
        f.write(response.content)
    
    print(f"ğŸ‰ ì €ì¥ ì™„ë£Œ: {filename}")
    return filename

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    url = "https://shopping.naver.com/example-product"
    filename = generate_shorts(url, "executive-fox")
    print(f"\nâœ… ì‡¼ì¸  ìƒì„± ì™„ë£Œ: {filename}")
```

**ì‹¤í–‰:**
```bash
python generate_shorts.py
```

---

### **ë°©ë²• 3: Swagger UI (ë¸Œë¼ìš°ì €)**

1. **ë¸Œë¼ìš°ì € ì—´ê¸°**: http://localhost:8000/docs
2. **POST /api/shorts/generate** í´ë¦­
3. **Try it out** í´ë¦­
4. **Request body** ì…ë ¥:
   ```json
   {
     "url": "https://example.com/product",
     "character_id": "executive-fox",
     "duration": 15
   }
   ```
5. **Execute** í´ë¦­
6. **Response** ì—ì„œ `job_id` ë³µì‚¬
7. **GET /api/shorts/status/{job_id}** ë¡œ ìƒíƒœ í™•ì¸
8. **GET /api/shorts/download/{job_id}** ë¡œ ë‹¤ìš´ë¡œë“œ

---

## ğŸ“Š ì„±ëŠ¥ ë° ì‹œê°„

### **RTX 3060 12GB ê¸°ì¤€**

```
ë‹¨ê³„                        ì‹œê°„
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
1. ì›¹ í¬ë¡¤ë§                5ì´ˆ
2. ìŠ¤í¬ë¦½íŠ¸ ìƒì„± (LLaMA)    20ì´ˆ
3. ì´ë¯¸ì§€ ìƒì„± (5ì¥)        50ì´ˆ
4. ìŒì„± í•©ì„± (5ê°œ)          20ì´ˆ
5. ë¹„ë””ì˜¤ ìƒì„± (5ê°œ)        300ì´ˆ
6. ìµœì¢… ë Œë”ë§              60ì´ˆ
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ì´ ì†Œìš” ì‹œê°„:              ~7.5ë¶„
```

### **GPUë³„ ì˜ˆìƒ ì‹œê°„**

| GPU | VRAM | ì˜ˆìƒ ì‹œê°„ |
|-----|------|----------|
| RTX 4090 | 24GB | 4-5ë¶„ |
| RTX 4080 | 16GB | 5-6ë¶„ |
| RTX 3090 | 24GB | 6-7ë¶„ |
| **RTX 3060** | 12GB | **7-8ë¶„** |
| GTX 1660 Ti | 6GB | 10-12ë¶„ |
| CPU Only | - | 30-45ë¶„ |

---

## ğŸ› ï¸ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### **ë¬¸ì œ 1: CUDA Out of Memory**

**ì¦ìƒ:**
```
RuntimeError: CUDA out of memory. Tried to allocate 2.00 GiB
```

**í•´ê²°:**
```bash
# 1. ë‹¤ë¥¸ GPU ì‚¬ìš© í”„ë¡œê·¸ë¨ ì¢…ë£Œ
nvidia-smi  # GPU ì‚¬ìš© í™•ì¸

# 2. ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸° (backend/models/*.py ìˆ˜ì •)
# num_inference_steps = 30 â†’ 20

# 3. CPU Offloading í™œì„±í™” (ìë™ìœ¼ë¡œ ì„¤ì •ë¨)
```

---

### **ë¬¸ì œ 2: Ollama ì—°ê²° ì‹¤íŒ¨**

**ì¦ìƒ:**
```
âŒ Ollama API error: Connection refused
```

**í•´ê²°:**
```bash
# Ollama ì„œë²„ ì‹œì‘
ollama serve

# ë‹¤ë¥¸ í„°ë¯¸ë„ì—ì„œ í™•ì¸
curl http://localhost:11434/api/tags

# ëª¨ë¸ ì¬ë‹¤ìš´ë¡œë“œ
ollama pull llama3.1:8b
```

---

### **ë¬¸ì œ 3: ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨**

**ì¦ìƒ:**
```
HTTPError: 404 Client Error
```

**í•´ê²°:**
```bash
# Hugging Face í† í° ì„¤ì • (ì„ íƒ)
export HF_TOKEN="your_token_here"

# ìºì‹œ ì‚­ì œ í›„ ì¬ì‹œë„
rm -rf ~/.cache/huggingface
python scripts/install_models.py
```

---

### **ë¬¸ì œ 4: FFmpeg ì˜¤ë¥˜**

**ì¦ìƒ:**
```
FileNotFoundError: [Errno 2] No such file or directory: 'ffmpeg'
```

**í•´ê²°:**
```bash
# FFmpeg ì„¤ì¹˜ í™•ì¸
ffmpeg -version

# ì—†ìœ¼ë©´ ì„¤ì¹˜
# Linux: sudo apt install ffmpeg
# macOS: brew install ffmpeg
# Windows: PATHì— ffmpeg.exe ì¶”ê°€
```

---

## â“ FAQ

### **Q1: GPU ì—†ì´ ì‹¤í–‰ ê°€ëŠ¥í•œê°€ìš”?**

**A:** ë„¤, CPUë§Œìœ¼ë¡œë„ ì‹¤í–‰ ê°€ëŠ¥í•˜ì§€ë§Œ ë§¤ìš° ëŠë¦½ë‹ˆë‹¤ (30-45ë¶„/ì‡¼ì¸ ).

```bash
# PyTorch CPU ë²„ì „ ì„¤ì¹˜
pip install torch torchvision torchaudio
```

---

### **Q2: ë¹„ìš©ì´ ë“œë‚˜ìš”?**

**A:** ì•„ë‹ˆìš”! ì™¸ë¶€ API ë¹„ìš© ì—†ì´ ì „ê¸°ë£Œë§Œ ë°œìƒí•©ë‹ˆë‹¤ ($5-10/ì›”).

---

### **Q3: ì˜¤í”„ë¼ì¸ì—ì„œ ì‹¤í–‰ ê°€ëŠ¥í•œê°€ìš”?**

**A:** ë„¤! ëª¨ë¸ ë‹¤ìš´ë¡œë“œ í›„ ì™„ì „ ì˜¤í”„ë¼ì¸ ì‘ë™ ê°€ëŠ¥í•©ë‹ˆë‹¤.  
(ì›¹ í¬ë¡¤ë§ ê¸°ëŠ¥ë§Œ ì¸í„°ë„· í•„ìš”)

---

### **Q4: ìºë¦­í„° ì¶”ê°€ ê°€ëŠ¥í•œê°€ìš”?**

**A:** ë„¤! `backend/app.py`ì˜ `get_characters()` í•¨ìˆ˜ì—ì„œ ìºë¦­í„°ë¥¼ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---

### **Q5: ë‹¤ë¥¸ ì–¸ì–´ ì§€ì›í•˜ë‚˜ìš”?**

**A:** í˜„ì¬ í•œêµ­ì–´ TTSë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ, Coqui TTSëŠ” ë‹¤êµ­ì–´ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.

---

## ğŸ“š ì¶”ê°€ ìë£Œ

- **ì‹œìŠ¤í…œ ì„¤ê³„**: `LOCAL_PC_SHORTS_SYSTEM.md`
- **í”„ë¡œì íŠ¸ ì™„ì„± ë³´ê³ ì„œ**: `LOCAL_PC_SHORTS_COMPLETE.md`
- **ë¦¬ì†ŒìŠ¤ ê²€ì¦**: `LOCAL_PC_RESOURCE_VERIFICATION.md`
- **API ë¬¸ì„œ**: http://localhost:8000/docs

---

## ğŸ‰ ì™„ë£Œ!

ì´ì œ ë¡œì»¬ PCì—ì„œ AI ì‡¼ì¸ ë¥¼ ë¬´ì œí•œìœ¼ë¡œ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!

```bash
# ë¹ ë¥¸ ì‹œì‘
cd local-shorts-system
source venv/bin/activate
cd backend
python app.py

# â†’ http://localhost:8000
```

**ğŸŠ ì¦ê±°ìš´ ì‡¼ì¸  ì œì‘ ë˜ì„¸ìš”! ğŸŠ**

---

**ì‘ì„±ì¼**: 2025-12-25  
**ë²„ì „**: 1.0.0  
**ë¼ì´ì„ ìŠ¤**: MIT
